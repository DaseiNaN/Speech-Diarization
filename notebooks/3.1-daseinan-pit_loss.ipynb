{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from itertools import permutations\n",
    "\n",
    "# P: number of permutation\n",
    "# T: number of frames\n",
    "# C: number of speakers (classes)\n",
    "# B: mini-batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pit_loss(pred, label):\n",
    "    \"\"\"\n",
    "    Permutation-invariant training (PIT) cross entropy loss function.\n",
    "\n",
    "    Args:\n",
    "      pred:  (T,C)-shaped pre-activation values\n",
    "      label: (T,C)-shaped labels in {0,1}\n",
    "\n",
    "    Returns:\n",
    "      min_loss: (1,)-shape mean cross entropy\n",
    "      label_perms[min_index]: (T,C)-shaped permutated labels \n",
    "      sigma: (P,)-shaped permutation tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    T = len(label)\n",
    "    C = label.shape[-1]\n",
    "    \n",
    "    # 所有的可能的 permutation 构成的 list\n",
    "    label_perms_indices = [list(p) for p in permutations(range(C))]\n",
    "    P = len(label_perms_indices)\n",
    "    perm_mat = torch.zeros(P, T, C, C)\n",
    "\n",
    "    for i, p in enumerate(label_perms_indices):\n",
    "        perm_mat[i, :, torch.arange(label.shape[-1]), p] = 1\n",
    "\n",
    "    # 获得不同 permutation 下对应的 label\n",
    "    x = torch.unsqueeze(torch.unsqueeze(label, 0), -1)  # (1, T, C, 1)\n",
    "    y = torch.arange(P * T * C).view(P, T, C, 1)        # (P, T, C, 1)\n",
    "\n",
    "    broadcast_label = torch.broadcast_tensors(x, y)[0]  # (P, T, C, 1)\n",
    "    allperm_label = torch.matmul(\n",
    "            perm_mat, broadcast_label\n",
    "            ).squeeze(-1)                               # (P, T, C)\n",
    "\n",
    "    # 对 pred 进行 P 次复制\n",
    "    x = torch.unsqueeze(pred, 0)                        # (1, T, C)\n",
    "    y = torch.arange(P * T).view(P, T, 1)               # (P, T, 1)\n",
    "    broadcast_pred = torch.broadcast_tensors(x, y)[0]   # (P, T, C)\n",
    "\n",
    "    # 计算不同 permutation 下的二元交叉熵损失\n",
    "    # broadcast_pred: (P, T, C)\n",
    "    # allperm_label: (P, T, C)\n",
    "    losses = F.binary_cross_entropy_with_logits(\n",
    "               broadcast_pred,\n",
    "               allperm_label,\n",
    "               reduction='none')\n",
    "    mean_losses = torch.mean(torch.mean(losses, dim=1), dim=1)\n",
    "    min_loss = torch.min(mean_losses) * len(label)\n",
    "    min_index = torch.argmin(mean_losses)\n",
    "    \n",
    "    # sigma - 最优的 permutation\n",
    "    sigma = list(permutations(range(label.shape[-1])))[min_index]\n",
    "\n",
    "    return min_loss, allperm_label[min_index], sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20\n",
    "C = 2\n",
    "pred = np.random.randn(T, C)\n",
    "label = np.random.randn(T, C)\n",
    "pred = torch.from_numpy(pred).to(torch.float32)\n",
    "label = torch.from_numpy(label).to(torch.float32)\n",
    "\n",
    "print(\"pred.shape       = {}\".format(pred.shape))\n",
    "print(\"label.shape      = {}\".format(label.shape))\n",
    "\n",
    "min_loss, perm_label, sigma = pit_loss(pred, label)\n",
    "print(\"min_loss         = {:.2f}\".format(min_loss))\n",
    "print(\"perm_label.shape = {}\".format(perm_label.shape))\n",
    "print(\"sigma            = {}\".format(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_pit_loss(ys, ts, ilens=None):\n",
    "    \"\"\"\n",
    "    PIT loss over mini-batch.\n",
    "\n",
    "    Args:\n",
    "      ys: B-length list of predictions\n",
    "      ts: B-length list of labels\n",
    "\n",
    "    Returns:\n",
    "      loss: (1,)-shape mean cross entropy over mini-batch\n",
    "      labels: B-length list of permuted labels\n",
    "      sigmas: B-length list of permutation\n",
    "    \"\"\"\n",
    "    if ilens is None:\n",
    "        ilens = [t.shape[0] for t in ts]\n",
    "\n",
    "    loss_w_labels_w_sigmas = [pit_loss(y[:ilen, :], t[:ilen, :])\n",
    "                              for (y, t, ilen) in zip(ys, ts, ilens)]\n",
    "    losses, labels, sigmas = zip(*loss_w_labels_w_sigmas)\n",
    "    loss = torch.sum(torch.stack(losses))\n",
    "    n_frames = np.sum([ilen for ilen in ilens])\n",
    "    loss = loss / n_frames\n",
    "    return loss, labels, sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 64\n",
    "T = 20\n",
    "C = 2\n",
    "pred = np.random.randn(B, T, C)\n",
    "label = np.random.randn(B, T, C)\n",
    "pred = torch.from_numpy(pred).to(torch.float32)\n",
    "label = torch.from_numpy(label).to(torch.float32)\n",
    "\n",
    "print(\"pred.shape       = {}\".format(pred.shape))\n",
    "print(\"label.shape      = {}\".format(label.shape))\n",
    "\n",
    "min_loss, labels, sigma = batch_pit_loss(pred, label)\n",
    "print(\"min_loss         = {:.2f}\".format(min_loss))\n",
    "print(\"labels           = {} - {} x {}\".format(type(labels), len(labels), labels[0].shape))\n",
    "print(\"sigma            = {} - {} x {}\".format(type(sigma), len(sigma), len(sigma[0])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc151fd16db9b51e0a5384d3d4cd420e8b7fffa33a680ea423fdaa6eedf4e1b5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('diarization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
